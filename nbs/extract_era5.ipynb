{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA5 Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee \n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize(project='my-project-410920') ### YOUR PROJECT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_extract = ['temperature_2m', 'total_precipitation_sum', 'u_component_of_wind_10m',\n",
    "\t\t\t\t\t\t'v_component_of_wind_10m', 'surface_pressure', 'snowfall_sum',\n",
    "\t\t\t\t\t\t'snowmelt_sum', 'dewpoint_temperature_2m',\n",
    "    \t\t\t\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_df_sub1 = pd.read_csv('data/xy_df/xy_df_sub1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_to_coord = {}\n",
    "\n",
    "for ind, row in xy_df_sub1.iterrows():\n",
    "\tif (row['latitude']) != 0 or (row['longitude']) != 0:\n",
    "\t\tgrid_to_coord[row['grid_id']] = (row['latitude'], row['longitude']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in xy_df_sub1.iterrows():\n",
    "    if (row['latitude'] == 0) and (row['longitude'] == 0):\n",
    "        latitude, longitude = grid_to_coord.get(row['grid_id'], (None, None))\n",
    "        if latitude is None or longitude is None:\n",
    "            print('err')\n",
    "            break\n",
    "        xy_df_sub1.at[ind, 'latitude'] = latitude\n",
    "        xy_df_sub1.at[ind, 'longitude'] = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Earth Engine ImageCollection\n",
    "IC = ee.ImageCollection('ECMWF/ERA5_LAND/MONTHLY_AGGR').select(variables_to_extract)\n",
    "\n",
    "def process_grid_point(args):\n",
    "    '''\n",
    "    process entries of xy_df_sub1 and populate appropriate directories\n",
    "    '''\n",
    "    index, row = args\n",
    "    latitude_to_extract = row['latitude']\n",
    "    longitude_to_extract = row['longitude']\n",
    "    year_to_process = int(row['year'])\n",
    "    grid_id = row['grid_id']\n",
    "    flood_target = row[f'target_flood_{n_pred}']\n",
    "    point = ee.Geometry.Point(longitude_to_extract, latitude_to_extract)\n",
    "\n",
    "    # Create the output directory\n",
    "    output_dir = f'era5_new/target_flood/extracted_{index}' if flood_target == 1 else f'era5_new/no_flood_target/extracted_{index}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define the date range for the entire period (5 years)\n",
    "    start_date = datetime(year_to_process - 4, 1, 1)\n",
    "    end_date = datetime(year_to_process, 12, 31)\n",
    "\n",
    "    # Filter the ImageCollection for the entire date range\n",
    "    era5_tp = IC.filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    # Extract the time series for the entire period\n",
    "    try:\n",
    "        time_series = era5_tp.getRegion(point, scale=100000).getInfo()\n",
    "        time_series_df = pd.DataFrame(time_series[1:], columns=time_series[0])\n",
    "\n",
    "        # Convert the time column to a readable date format\n",
    "        time_series_df['time'] = pd.to_datetime(time_series_df['time'], unit='ms')\n",
    "\n",
    "        # Melt the DataFrame to the desired format\n",
    "        melted_df = time_series_df.melt(\n",
    "            id_vars=['time'],\n",
    "            value_vars=variables_to_extract,\n",
    "            var_name='id',\n",
    "            value_name='value'\n",
    "        )\n",
    "\n",
    "        # Sort by variable name (id) and then by time\n",
    "        melted_df = melted_df.sort_values(by=['id', 'time'])\n",
    "\n",
    "        # Save the entire melted DataFrame to a single CSV file\n",
    "        output_file = os.path.join(output_dir, f'era5_data.csv')\n",
    "        melted_df.to_csv(output_file, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process grid point {grid_id, row.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "\tlist(tqdm(executor.map(process_grid_point, [(index, row) for index, row in xy_df_sub1.iterrows()]), total=len(xy_df_sub1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dir(xy_df_sub1, path):\n",
    "\t'''\n",
    "\tread directories and flatten weather vars\n",
    "\t'''\n",
    "\tdirectories = sorted([f for f in os.listdir(path) if not f.startswith('.')])\n",
    "\tnum_features = 480\n",
    "\n",
    "\t# Create column names for the new features\n",
    "\tcolumn_names = [f'feature_{i}' for i in range(num_features)]\n",
    "\n",
    "\t# Create an empty list to store the rows of data\n",
    "\tdata = []\n",
    "\n",
    "\t# Loop through directories and process each one\n",
    "\tfor directory in tqdm(directories):\n",
    "\t\t# Extract index from directory name\n",
    "\t\tidx = int(directory.split('_')[1])\n",
    "\n",
    "\t\t# Read and flatten features\n",
    "\t\tfeatures_df = pd.read_csv(f'{path}/{directory}/era5_data.csv')\n",
    "\t\tvalues = list(features_df['value'])\n",
    "\n",
    "\t\t# Gather the entire row's data (keep all existing columns)\n",
    "\t\trow_data = xy_df_sub1.loc[idx].values  # All columns in that row\n",
    "\n",
    "\t\t# Append the original row data + the new flattened feature values\n",
    "\t\tdata.append(list(row_data) + list(values))\n",
    "\n",
    "\t# Create a new DataFrame with all original columns and the new features\n",
    "\treturn pd.DataFrame(data, columns=xy_df_sub1.columns.tolist() + column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_df_sub_new_no_flood = populate_dir(xy_df_sub1, 'data/era5/target_1/no_flood_target')\n",
    "xy_df_sub_new_flood = populate_dir(xy_df_sub1, 'data/era5/target_1/target_flood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_df_sub1_new_combined = pd.concat([xy_df_sub_new_no_flood, xy_df_sub_new_flood], axis=0)\n",
    "xy_df_sub1_new_combined.to_csv('xy_df_sub_1_combined.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
