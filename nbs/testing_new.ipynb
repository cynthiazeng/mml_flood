{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#run this in colab to downgrade sklearn\n",
        "!pip install scikit-learn==1.3.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gDO97Gy-mmT",
        "outputId": "89a4e621-1a23-42aa-de03-5d34d27a6288"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.1 in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive and change to the appropriate directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd drive/My Drive/mml_flood/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUYiGm-a_Z3j",
        "outputId": "1fe581bc-0980-4f2f-f4c9-cb4a8361deee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/mml_flood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZX6i8_iF-ROn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "#split training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import utils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attach_target(x_df, y_master, disaster, next_n):\n",
        "    \"\"\"\n",
        "    Attach a 'target' column to x_df based on disaster data in y_master.\n",
        "    If no data exists in y_master for a given grid_id and year, set the target to NaN.\n",
        "    Look up from next 1 to next_n years, if there is a major flood occuring.\n",
        "    \"\"\"\n",
        "    # Create a dictionary for fast lookup: {(grid_id, year): disaster_value}\n",
        "    disaster_lookup = {\n",
        "        (row['grid_id'], row['year']): row[disaster+'_bin']\n",
        "        for _, row in y_master.iterrows()\n",
        "    }\n",
        "\n",
        "    # Initialize a 'target' column in x_df\n",
        "    target_col = 'target_' + disaster + '_' + str(next_n)\n",
        "    x_df[target_col] = np.nan  # Default to NaN\n",
        "\n",
        "    # Iterate over x_df rows\n",
        "    for idx, row in x_df.iterrows():\n",
        "        grid_id = row['grid_id']\n",
        "        year = row['year']\n",
        "\n",
        "        # Check if (grid_id, year + next_n + 1) exists in y_master\n",
        "        if (grid_id, year + next_n ) not in disaster_lookup:\n",
        "            # No data found for (grid_id, year + next_n + 1), skip this row\n",
        "            continue\n",
        "\n",
        "        # Check years from year+1 to year+next_n\n",
        "        target_found = 0\n",
        "        for i in range(1, next_n + 1):\n",
        "            future_year = year + i\n",
        "            if disaster_lookup.get((grid_id, future_year), 0) == 1:\n",
        "                target_found = 1\n",
        "                break\n",
        "\n",
        "        # Update the 'target' column\n",
        "        x_df.at[idx, target_col] = target_found\n",
        "    # Drop rows where 'target' is NaN\n",
        "    x_df = x_df.dropna(subset=[target_col])\n",
        "\n",
        "    return x_df\n",
        "\n",
        "# #attach target for a particular disease for next n years, using y_master\n",
        "# #next_n is how we choose the next n-periods for the prediction target\n",
        "# def attach_target_old(x_df, y_master, disaster, next_n):\n",
        "#     y = y_master.copy()\n",
        "#     #shift years\n",
        "#     y['year'] = y['year'] - next_n\n",
        "#     #keep for particular disaster\n",
        "#     y = y[['grid_id','year',disaster+'_bin']]\n",
        "#     # Rename into target\n",
        "#     y = y.rename(columns={disaster +'_bin': 'target_' + disaster + '_'+ str(next_n)})\n",
        "#     xy_df = pd.merge(x_df, y, on = ['grid_id','year'], how='inner')\n",
        "#     return xy_df\n"
      ],
      "metadata": {
        "id": "nLUo0bm7-Sdi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data\n",
        "x_df = pd.read_csv('data/xy_df/x_stat.csv')  # Set index=False to avoid saving the index as a column\n",
        "y_master = pd.read_csv('data/xy_df/y_master.csv')\n",
        "print(x_df.shape, y_master.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7X_Rj4C-0_I",
        "outputId": "2f2894d0-e7c8-4517-98d8-39349546595e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48970, 28) (166793, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Riley: you can add attach other features here using some old codes, but be careful with merging, and name the features with 'nlp_' and 'era_'\n",
        "# check there is no redundant features\n",
        "\n",
        "# #attach nlp to xy_df\n",
        "# def attach_nlp(xy_df, df_nlp):\n",
        "#     #correct formatting for df_nlp\n",
        "#     df_nlp = string_to_tuple(df_nlp, 'grid_id')\n",
        "#     #drop text, location and label columns\n",
        "#     df_nlp = df_nlp.drop(['location','txt','label','flood_ct_x'], axis=1)\n",
        "#     #add prefix\n",
        "#     df_nlp = df_nlp.rename(columns={c: 'nlp_' + str(c) for c in df_nlp.columns if c not in ['grid_id']})\n",
        "#     #merge\n",
        "#     xy_df_out = pd.merge(xy_df, df_nlp , on='grid_id', how='left')\n",
        "#     print('shape of xy_df with nlp features', xy_df_out.shape)\n",
        "#     return xy_df_out\n",
        "\n",
        "# #format function for above\n",
        "# def string_to_tuple(df, col):\n",
        "#     try:\n",
        "#         df[col] = df.apply(lambda row: eval(row[col]), axis=1)\n",
        "#     except:\n",
        "#         'error converting to tuple'\n",
        "#     return df\n",
        "\n",
        "# #attach era features\n",
        "# def attach_era(xy_df, df_era):\n",
        "#     #add prefix\n",
        "#     df_era = df_era.rename(columns={c: 'era_' + c for c in df_era.columns if c not in ['grid_id','year']})\n",
        "#     xy_df_out = pd.merge(xy_df, df_era, on=['grid_id','year'], how='left')\n",
        "#     print('shape of xy_df with era features', xy_df_out.shape)\n",
        "#     return xy_df_out\n",
        "\n"
      ],
      "metadata": {
        "id": "oLdkb9gu-9l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file named 'x'\n",
        "# df_nlp = pd.read_csv('data/nlp/df_cls_transfer128.csv')\n",
        "\n",
        "# Read era features"
      ],
      "metadata": {
        "id": "QV8Lig2h_nwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#construct xy_df for appropriate prediction year, depending on the n_pred target period\n",
        "n_pred = 5\n",
        "\n",
        "#Riley: attach NLP and ERA features here\n",
        "\n",
        "# x_df = x_df.loc[x_df['year']>=1979] #crop to after 1979\n",
        "xy_df = attach_target(x_df, y_master, 'flood', n_pred)\n",
        "print('length of xy_df', len(xy_df))\n",
        "print('imbalance', xy_df.filter(regex='target').sum()/len(xy_df))\n",
        "\n",
        "xy_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9X3yPye_oSZ",
        "outputId": "453781b8-324d-4225-dc0c-ae00793c5a2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of xy_df 44820\n",
            "imbalance target_flood_5    0.243351\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['grid_id', 'year', 'stat_flood_amt', 'stat_storm_amt',\n",
              "       'stat_earthquake_amt', 'stat_extreme temperature _amt',\n",
              "       'stat_landslide_amt', 'stat_volcanic activity_amt', 'stat_drought_amt',\n",
              "       'stat_mass movement (dry)_amt', 'stat_flood_ct', 'stat_storm_ct',\n",
              "       'stat_earthquake_ct', 'stat_extreme temperature _ct',\n",
              "       'stat_landslide_ct', 'stat_volcanic activity_ct', 'stat_drought_ct',\n",
              "       'stat_mass movement (dry)_ct', 'stat_flood_bin', 'stat_storm_bin',\n",
              "       'stat_earthquake_bin', 'stat_extreme temperature _bin',\n",
              "       'stat_landslide_bin', 'stat_volcanic activity_bin', 'stat_drought_bin',\n",
              "       'stat_mass movement (dry)_bin', 'stat_lat', 'stat_lon',\n",
              "       'target_flood_5'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random splitting\n",
        "results={}\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "\n",
        "\n",
        "#train_test_split randomly\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "print(\"data imbalance train\", y_train.sum()/len(y_train))\n",
        "print(\"data imbalance test\", y_test.sum()/len(y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train, y_train, x_test)\n",
        "results['stats only random split'] = utils.get_scores_clf(y_test, y_pred_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxHMcTRB_yAg",
        "outputId": "fd692c14-59a2-4b98-9e17-e8ea621b9e84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8563974915055976\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.6193620013919782 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.8162112347894114 0.6193620013919782 0.6326788636025584 0.73467759079584 0.3781665059592997 0.9304347826086956\n",
            "[[5511 4715]\n",
            " [ 224 2996]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Riley:\n",
        "# - fix the linking problem of NLP features (pls use the old .pkl let's make sure we are using as raw as possible)\n",
        "# - attach era features\n",
        "# - compute results for random split, and non-random split: using the NEW attach_target function here\n",
        "# - for each, record the data imbalance issues.\n",
        "\n",
        "\n",
        "# - run results training using ALL data, and compute the locations with the highest risks of flooding with n_pred = 3, 5, that is 2021 and 2023 -> this is the closest to a \"live\" prediction we can do using our data.\n",
        "# - let's think about how to visualize the live.\n",
        "# - we should discuss the paper -> given current results, the paper is not very strong... option 1: we don't submit, keep working on it. option 2: we write something about the observations and try submit.\n"
      ],
      "metadata": {
        "id": "PzgNI-4t_plO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}