{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gDO97Gy-mmT",
        "outputId": "89a4e621-1a23-42aa-de03-5d34d27a6288"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "%pip install scikit-learn==1.3.1 \n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZX6i8_iF-ROn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "#split training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nLUo0bm7-Sdi"
      },
      "outputs": [],
      "source": [
        "def attach_target(x_df, y_master, disaster, next_n):\n",
        "    \"\"\"\n",
        "    Attach a 'target' column to x_df based on disaster data in y_master.\n",
        "    If no data exists in y_master for a given grid_id and year, set the target to NaN.\n",
        "    Look up from next 1 to next_n years, if there is a major flood occuring.\n",
        "    \"\"\"\n",
        "    \n",
        "    x_df = x_df.copy()\n",
        "    # Create a dictionary for fast lookup: {(grid_id, year): disaster_value}\n",
        "    disaster_lookup = {\n",
        "        (row['grid_id'], row['year']): row[disaster+'_bin']\n",
        "        for _, row in y_master.iterrows()\n",
        "    }\n",
        "\n",
        "    # Initialize a 'target' column in x_df\n",
        "    target_col = 'target_' + disaster + '_' + str(next_n)\n",
        "    x_df[target_col] = np.nan  # Default to NaN\n",
        "\n",
        "    # Iterate over x_df rows\n",
        "    for idx, row in x_df.iterrows():\n",
        "        grid_id = row['grid_id']\n",
        "        year = row['year']\n",
        "\n",
        "        # Check if (grid_id, year + next_n + 1) exists in y_master\n",
        "        if (grid_id, year + next_n ) not in disaster_lookup:\n",
        "            # No data found for (grid_id, year + next_n + 1), skip this row\n",
        "            continue\n",
        "\n",
        "        # Check years from year+1 to year+next_n\n",
        "        target_found = 0\n",
        "        for i in range(1, next_n + 1):\n",
        "            future_year = year + i\n",
        "            if disaster_lookup.get((grid_id, future_year), 0) == 1:\n",
        "                target_found = 1\n",
        "                break\n",
        "\n",
        "        # Update the 'target' column\n",
        "        x_df.at[idx, target_col] = target_found\n",
        "    # Drop rows where 'target' is NaN\n",
        "    x_df = x_df.dropna(subset=[target_col])\n",
        "\n",
        "    return x_df\n",
        "\n",
        "# #attach target for a particular disease for next n years, using y_master\n",
        "# #next_n is how we choose the next n-periods for the prediction target\n",
        "# def attach_target_old(x_df, y_master, disaster, next_n):\n",
        "#     y = y_master.copy()\n",
        "#     #shift years\n",
        "#     y['year'] = y['year'] - next_n\n",
        "#     #keep for particular disaster\n",
        "#     y = y[['grid_id','year',disaster+'_bin']]\n",
        "#     # Rename into target\n",
        "#     y = y.rename(columns={disaster +'_bin': 'target_' + disaster + '_'+ str(next_n)})\n",
        "#     xy_df = pd.merge(x_df, y, on = ['grid_id','year'], how='inner')\n",
        "#     return xy_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7X_Rj4C-0_I",
        "outputId": "2f2894d0-e7c8-4517-98d8-39349546595e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(48970, 29) (166793, 11)\n"
          ]
        }
      ],
      "source": [
        "#Read data\n",
        "x_df = pd.read_csv('data/testing/x_stat.csv')  # Set index=False to avoid saving the index as a column\n",
        "y_master = pd.read_csv('data/testing/y_master.csv')\n",
        "print(x_df.shape, y_master.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_stat = pd.read_csv('data/testing/x_stat.csv')\n",
        "x_nlp = pd.read_csv('data/testing/x_nlp.csv')\n",
        "x_era = pd.read_csv('data/testing/x_era.csv')\n",
        "x_full = pd.read_csv('data/testing/x_full.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def get_scores_clf(y_true, y_pred_prob):\n",
        "    # Get F1 scores at different thresholds\n",
        "    f1_scores = []\n",
        "    thres_list = [0.4, 0.5, 0.6, 0.7]\n",
        "    \n",
        "    for thres in thres_list:\n",
        "        y_pred = (y_pred_prob >= thres).astype(int)\n",
        "        f1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
        "        f1_scores.append(f1)\n",
        "    \n",
        "    # Find best threshold\n",
        "    max_f1 = max(f1_scores)\n",
        "    max_thres = thres_list[f1_scores.index(max_f1)]\n",
        "    # print('Max F1 Score:', max_f1, 'at threshold:', max_thres)\n",
        "\n",
        "    # Get y_pred using the best threshold\n",
        "    y_pred = (y_pred_prob >= max_thres).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    accu = metrics.accuracy_score(y_true, y_pred)\n",
        "    accu_bl = metrics.balanced_accuracy_score(y_true, y_pred)\n",
        "    auc = metrics.roc_auc_score(y_true, y_pred_prob)  # No need for `multi_class='ovo'` unless multi-class\n",
        "    precision = metrics.precision_score(y_true, y_pred)\n",
        "    recall = metrics.recall_score(y_true, y_pred)\n",
        "\n",
        "    # print(f'AUC: {auc:.4f}, F1: {max_f1:.4f}, Accuracy: {accu:.4f}, Balanced Acc: {accu_bl:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
        "    # print('Confusion Matrix:\\n', metrics.confusion_matrix(y_true, y_pred))\n",
        "    \n",
        "    return auc, max_f1, accu, accu_bl, precision, recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Chronological Split n=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 1\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "x = x.filter(regex='stat|year')\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>stat_flood_amt</th>\n",
              "      <th>stat_storm_amt</th>\n",
              "      <th>stat_earthquake_amt</th>\n",
              "      <th>stat_extreme temperature _amt</th>\n",
              "      <th>stat_landslide_amt</th>\n",
              "      <th>stat_volcanic activity_amt</th>\n",
              "      <th>stat_drought_amt</th>\n",
              "      <th>stat_mass movement (dry)_amt</th>\n",
              "      <th>stat_flood_ct</th>\n",
              "      <th>...</th>\n",
              "      <th>stat_flood_bin</th>\n",
              "      <th>stat_storm_bin</th>\n",
              "      <th>stat_earthquake_bin</th>\n",
              "      <th>stat_extreme temperature _bin</th>\n",
              "      <th>stat_landslide_bin</th>\n",
              "      <th>stat_volcanic activity_bin</th>\n",
              "      <th>stat_drought_bin</th>\n",
              "      <th>stat_mass movement (dry)_bin</th>\n",
              "      <th>stat_lat</th>\n",
              "      <th>stat_lon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1960</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-46</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-46</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1962</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-46</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1963</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-46</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1964</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-46</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  stat_flood_amt  stat_storm_amt  stat_earthquake_amt  \\\n",
              "0  1960             0.0             0.0                  0.0   \n",
              "1  1961             0.0             0.0                  0.0   \n",
              "2  1962             0.0             0.0                  0.0   \n",
              "3  1963             0.0             0.0                  0.0   \n",
              "4  1964             0.0             0.0                  0.0   \n",
              "\n",
              "   stat_extreme temperature _amt  stat_landslide_amt  \\\n",
              "0                            0.0                 0.0   \n",
              "1                            0.0                 0.0   \n",
              "2                            0.0                 0.0   \n",
              "3                            0.0                 0.0   \n",
              "4                            0.0                 0.0   \n",
              "\n",
              "   stat_volcanic activity_amt  stat_drought_amt  stat_mass movement (dry)_amt  \\\n",
              "0                         0.0               0.0                           0.0   \n",
              "1                         0.0               0.0                           0.0   \n",
              "2                         0.0               0.0                           0.0   \n",
              "3                         0.0               0.0                           0.0   \n",
              "4                         0.0               0.0                           0.0   \n",
              "\n",
              "   stat_flood_ct  ...  stat_flood_bin  stat_storm_bin  stat_earthquake_bin  \\\n",
              "0            0.0  ...             0.0             0.0                  0.0   \n",
              "1            0.0  ...             0.0             0.0                  0.0   \n",
              "2            0.0  ...             0.0             0.0                  0.0   \n",
              "3            0.0  ...             0.0             0.0                  0.0   \n",
              "4            0.0  ...             0.0             0.0                  0.0   \n",
              "\n",
              "   stat_extreme temperature _bin  stat_landslide_bin  \\\n",
              "0                            0.0                 0.0   \n",
              "1                            0.0                 0.0   \n",
              "2                            0.0                 0.0   \n",
              "3                            0.0                 0.0   \n",
              "4                            0.0                 0.0   \n",
              "\n",
              "   stat_volcanic activity_bin  stat_drought_bin  stat_mass movement (dry)_bin  \\\n",
              "0                         0.0               0.0                           0.0   \n",
              "1                         0.0               0.0                           0.0   \n",
              "2                         0.0               0.0                           0.0   \n",
              "3                         0.0               0.0                           0.0   \n",
              "4                         0.0               0.0                           0.0   \n",
              "\n",
              "   stat_lat  stat_lon  \n",
              "0       -46       168  \n",
              "1       -46       168  \n",
              "2       -46       168  \n",
              "3       -46       168  \n",
              "4       -46       168  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = sorted(x[\"year\"].unique())\n",
        "X = x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Accuracy over 12 iterations: 0.8755\n",
            "\n",
            "Average AUC over 12 iterations: 0.6027\n",
            "\n",
            "Average accu_bl over 12 iterations: 0.5143\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score  \n",
        "from sklearn import metrics\n",
        "\n",
        "verbose = False\n",
        "results = []\n",
        "aucs = []\n",
        "acc_bls = []\n",
        "for i in range(45,len(years) - 1):\n",
        "    train_years = years[: i + 1]\n",
        "    test_year = years[i + 1]\n",
        "    \n",
        "    X_train = X[x[\"year\"].isin(train_years)]\n",
        "    y_train = y[x[\"year\"].isin(train_years)]\n",
        "    \n",
        "    X_test = X[x[\"year\"] == test_year]\n",
        "    y_test = y[x[\"year\"] == test_year]\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "\t\tn_estimators=100,  \n",
        "\t\tmax_depth=4,       \n",
        "\t\trandom_state=42,\n",
        "\t)\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append(acc)\n",
        "    \n",
        "    y_prob = model.predict_proba(X_test)[:, 1]  \n",
        "\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    aucs.append(auc)\n",
        "    \n",
        "    \n",
        "    auc, max_f1, accu, accu_bl, precision, recall = get_scores_clf(y_test, y_prob)  \n",
        "    acc_bls.append(accu_bl)\n",
        "    if verbose:\n",
        "    \tprint(f\"Train up to {train_years[-1]}, test on {test_year}: AUC = {auc:.4f}, Balanced Acc = {accu_bl:.4f}\\n\")\n",
        "\n",
        "\t\n",
        "\n",
        "# Compute average performance\n",
        "avg_acc = np.mean(results)\n",
        "print(f\"\\nAverage Accuracy over {len(results)} iterations: {avg_acc:.4f}\")\n",
        "\n",
        "avg_auc = np.mean(auc)\n",
        "print(f\"\\nAverage AUC over {len(results)} iterations: {avg_auc:.4f}\")\n",
        "\n",
        "avg_bl = np.mean(acc_bls)\n",
        "print(f\"\\nAverage accu_bl over {len(results)} iterations: {avg_bl:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Chronological Split n=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 2\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "x = x.filter(regex='stat|year')\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = sorted(x[\"year\"].unique())\n",
        "X = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Accuracy over 11 iterations: 0.7802\n",
            "\n",
            "Average AUC over 11 iterations: 0.6530\n",
            "\n",
            "Average accu_bl over 11 iterations: 0.5435\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score  \n",
        "from sklearn import metrics\n",
        "\n",
        "verbose = False\n",
        "results = []\n",
        "aucs = []\n",
        "acc_bls = []\n",
        "for i in range(45,len(years) - 1):\n",
        "    train_years = years[: i + 1]\n",
        "    test_year = years[i + 1]\n",
        "    \n",
        "    X_train = X[x[\"year\"].isin(train_years)]\n",
        "    y_train = y[x[\"year\"].isin(train_years)]\n",
        "    \n",
        "    X_test = X[x[\"year\"] == test_year]\n",
        "    y_test = y[x[\"year\"] == test_year]\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "\t\tn_estimators=100,  \n",
        "\t\tmax_depth=4,       \n",
        "\t\trandom_state=42,\n",
        "\t)\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append(acc)\n",
        "    \n",
        "    y_prob = model.predict_proba(X_test)[:, 1]  \n",
        "\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    aucs.append(auc)\n",
        "    \n",
        "    \n",
        "    auc, max_f1, accu, accu_bl, precision, recall = get_scores_clf(y_test, y_prob)  \n",
        "    acc_bls.append(accu_bl)\n",
        "    if verbose:\n",
        "    \tprint(f\"Train up to {train_years[-1]}, test on {test_year}: AUC = {auc:.4f}, Balanced Acc = {accu_bl:.4f}\\n\")\n",
        "\n",
        "\t\n",
        "\n",
        "# Compute average performance\n",
        "avg_acc = np.mean(results)\n",
        "print(f\"\\nAverage Accuracy over {len(results)} iterations: {avg_acc:.4f}\")\n",
        "\n",
        "avg_auc = np.mean(auc)\n",
        "print(f\"\\nAverage AUC over {len(results)} iterations: {avg_auc:.4f}\")\n",
        "\n",
        "avg_bl = np.mean(acc_bls)\n",
        "print(f\"\\nAverage accu_bl over {len(results)} iterations: {avg_bl:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Chrono Split n=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 5\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "x = x.filter(regex='stat|year')\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = sorted(x[\"year\"].unique())\n",
        "X = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Accuracy over 8 iterations: 0.5994\n",
            "\n",
            "Average AUC over 8 iterations: 0.6298\n",
            "\n",
            "Average accu_bl over 8 iterations: 0.5841\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score  \n",
        "from sklearn import metrics\n",
        "\n",
        "verbose = False\n",
        "results = []\n",
        "aucs = []\n",
        "acc_bls = []\n",
        "for i in range(45,len(years) - 1):\n",
        "    train_years = years[: i + 1]\n",
        "    test_year = years[i + 1]\n",
        "    \n",
        "    X_train = X[x[\"year\"].isin(train_years)]\n",
        "    y_train = y[x[\"year\"].isin(train_years)]\n",
        "    \n",
        "    X_test = X[x[\"year\"] == test_year]\n",
        "    y_test = y[x[\"year\"] == test_year]\n",
        "    \n",
        "    model = XGBClassifier(\n",
        "\t\tn_estimators=100,  \n",
        "\t\tmax_depth=4,       \n",
        "\t\trandom_state=42,\n",
        "\t)\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append(acc)\n",
        "    \n",
        "    y_prob = model.predict_proba(X_test)[:, 1]  \n",
        "\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    aucs.append(auc)\n",
        "    \n",
        "    \n",
        "    auc, max_f1, accu, accu_bl, precision, recall = get_scores_clf(y_test, y_prob)  \n",
        "    acc_bls.append(accu_bl)\n",
        "    if verbose:\n",
        "    \tprint(f\"Train up to {train_years[-1]}, test on {test_year}: AUC = {auc:.4f}, Balanced Acc = {accu_bl:.4f}\\n\")\n",
        "\n",
        "\t\n",
        "\n",
        "# Compute average performance\n",
        "avg_acc = np.mean(results)\n",
        "print(f\"\\nAverage Accuracy over {len(results)} iterations: {avg_acc:.4f}\")\n",
        "\n",
        "avg_auc = np.mean(auc)\n",
        "print(f\"\\nAverage AUC over {len(results)} iterations: {avg_auc:.4f}\")\n",
        "\n",
        "avg_bl = np.mean(acc_bls)\n",
        "print(f\"\\nAverage accu_bl over {len(results)} iterations: {avg_bl:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n = 1, random split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "QV8Lig2h_nwr"
      },
      "outputs": [],
      "source": [
        "n_pred = 1\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7911262494579957\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5698252150104206 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.7525487631871196 0.5426665221788759 0.8769561002631214 0.5907154032189088 0.08909886453115956 0.26521239954075776\n",
            "[[12434  1137]\n",
            " [  640   231]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train1 = x_train1.filter(regex='stat|year')\n",
        "x_test1 = x_test1.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 stats only random split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6713414097619406\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.5466504015418363 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5369651349313865 0.5466504015418363 0.8944744495222269 0.5479274244287876 0.07341540970993071 0.15384615384615385\n",
            "[[12784   787]\n",
            " [  737   134]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "x_train1 = x_train1.filter(regex='nlp')\n",
        "x_test1 = x_test1.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 nlp only random split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9834871637822662\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5169620951127145 0.5\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.572730854380597 0.48661111102779125 0.9133084060379448 0.5149720299947353 0.0634174842318748 0.06199770378874857\n",
            "[[13136   435]\n",
            " [  817    54]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "x_train1 = x_train1.filter(regex='era_')\n",
        "x_test1 = x_test1.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 era only random split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9828030575691103\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5268744042133072 0.5\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5816528474093937 0.4877150937297996 0.91088491898629 0.5233522873832489 0.06595292505727865 0.08266360505166476\n",
            "[[13083   488]\n",
            " [  799    72]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "x_train1 = x_train1.filter(regex='nlp|era_')\n",
        "x_test1 = x_test1.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "# y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "# results['n=1 weather + nlp random split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n",
        "x_train1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7000428026211769\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5574530315529511 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.548130464256488 0.530418864868458 0.9121312837557125 0.5508759434266745 0.07848007482625628 0.14006888633754305\n",
            "[[13051   520]\n",
            " [  749   122]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "x_train1 = x_train1.filter(regex='stat|nlp')\n",
        "x_test1 = x_test1.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 stat + nlp random split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8702576843580211\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5435539601803967 0.5\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.6018614014604147 0.5061882051401289 0.8930895997784241 0.5450417208775957 0.07210502285208106 0.14925373134328357\n",
            "[[12768   803]\n",
            " [  741   130]]\n"
          ]
        }
      ],
      "source": [
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "x_train1 = x_train1.filter(regex='stat|era')\n",
        "x_test1 = x_test1.filter(regex='stat|era')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 stat + era random split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.063387\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.06031\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8817620087043468\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5676250250846085 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.7444174833873236 0.533746139219117 0.8885888381110649 0.5775655287778922 0.08551068412237006 0.22388059701492538\n",
            "[[12638   933]\n",
            " [  676   195]]\n"
          ]
        }
      ],
      "source": [
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 all vars random split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n = 1, chrono split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 1\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "\n",
        "split_year = 2015\n",
        "years = x['year']\n",
        "\n",
        "train_mask = years <= split_year\n",
        "test_mask = years > split_year\n",
        "\n",
        "x_train1, x_test1 = x[train_mask], x[test_mask]\n",
        "y_train1, y_test1 = y[train_mask], y[test_mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target_flood_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48850</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48908</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48909</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48967</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48968</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1660 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       target_flood_1\n",
              "56                0.0\n",
              "57                0.0\n",
              "115               0.0\n",
              "116               0.0\n",
              "174               0.0\n",
              "...               ...\n",
              "48850             0.0\n",
              "48908             0.0\n",
              "48909             0.0\n",
              "48967             0.0\n",
              "48968             0.0\n",
              "\n",
              "[1660 rows x 1 columns]"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7910331133816664\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5364854802680565 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5970479423318966 0.5327891922319167 0.772289156626506 0.5662305961325756 0.11300065986847377 0.31210191082802546\n",
            "[[1233  270]\n",
            " [ 108   49]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train1_stat = x_train1.filter(regex='stat|year')\n",
        "x_test1_stat = x_test1.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_stat, y_train1, x_test1_stat)\n",
        "results['n=1 stats only chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6546993497955295\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.09336053851689241 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.4877273054739778 0.09336053851689241 0.09698795180722891 0.43288158290637413 0.08452548667910544 0.8471337579617835\n",
            "[[  28 1475]\n",
            " [  24  133]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train1_nlp = x_train1.filter(regex='nlp')\n",
        "x_test1_nlp = x_test1.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_nlp, y_train1, x_test1_nlp)\n",
        "results['n=1 nlp only chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9518395157042275\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.4148354090481343 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5388077348487739 0.4148354090481343 0.5150602409638554 0.5240029495149827 0.0990527114076388 0.535031847133758\n",
            "[[771 732]\n",
            " [ 73  84]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train1_era = x_train1.filter(regex='era_')\n",
        "x_test1_era = x_test1.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_era, y_train1, x_test1_era)\n",
        "results['n=1 era only chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9527875516202837\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.41345612222426215 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5225175975013879 0.41345612222426215 0.5234939759036145 0.5029918930716062 0.09509660245035778 0.47770700636942676\n",
            "[[794 709]\n",
            " [ 82  75]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train1_wn = x_train1.filter(regex='nlp|era_')\n",
        "x_test1_wn = x_test1.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_wn, y_train1, x_test1_wn)\n",
        "results['n=1 weather + nlp chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6780925727107133\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5445568760611205 0.5\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5087553131528875 0.4936779059449867 0.8174698795180723 0.5541041060130271 0.11075751612773774 0.22929936305732485\n",
            "[[1321  182]\n",
            " [ 121   36]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train1_sn = x_train1.filter(regex='stat|nlp')\n",
        "x_test1_sn = x_test1.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_sn, y_train1, x_test1_sn)\n",
        "results['n=1 stat + nlp chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8435580459284815\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5400376437226346 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5673027617800492 0.4900098311357853 0.8662650602409638 0.5354174877421378 0.1067946033451367 0.12738853503184713\n",
            "[[1418   85]\n",
            " [ 137   20]]\n"
          ]
        }
      ],
      "source": [
        "x_train1_se = x_train1.filter(regex='stat|era')\n",
        "x_test1_se = x_test1.filter(regex='stat|era')\n",
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1_se, y_train1, x_test1_se)\n",
        "results['n=1 stat + era chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_1    0.061317\n",
            "dtype: float64\n",
            "data imbalance test target_flood_1    0.094578\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8570720894604147\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.516484403839055 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5655186442401821 0.516484403839055 0.8783132530120482 0.5164024392827933 0.09945480181014962 0.07006369426751592\n",
            "[[1447   56]\n",
            " [ 146   11]]\n"
          ]
        }
      ],
      "source": [
        "print(\"data imbalance train\", y_train1.sum()/len(y_train1))\n",
        "print(\"data imbalance test\", y_test1.sum()/len(y_test1))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train1, y_train1, x_test1)\n",
        "results['n=1 all vars chrono split'] = utils.get_scores_clf(y_test1, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n = 2, random split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 2\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target_flood_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5531</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13931</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28402</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37898</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31050</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15118</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13222</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4018</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22448</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10747</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14193 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       target_flood_2\n",
              "5531              0.0\n",
              "13931             0.0\n",
              "28402             0.0\n",
              "37898             0.0\n",
              "31050             0.0\n",
              "...               ...\n",
              "15118             0.0\n",
              "13222             0.0\n",
              "4018              0.0\n",
              "22448             0.0\n",
              "10747             0.0\n",
              "\n",
              "[14193 rows x 1 columns]"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8327027473086133\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.6021038607830143 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.7696441336855753 0.6021038607830143 0.7648840977946875 0.6639879341981094 0.19242711190723472 0.5320167564332735\n",
            "[[9967 2555]\n",
            " [ 782  889]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='stat|year')\n",
        "x_test2 = x_test2.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 stats only random split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6707777295889287\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5631960378939944 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5569996447186525 0.5631960378939944 0.8269569506094554 0.5604446407715599 0.14302304134590854 0.2118491921005386\n",
            "[[11383  1139]\n",
            " [ 1317   354]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='nlp')\n",
        "x_test2 = x_test2.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 nlp only random split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9528539329527689\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.541832744494412 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5805819818161329 0.5042268743606272 0.8073698301979849 0.5423432855122919 0.13196365530396267 0.19569120287253142\n",
            "[[11132  1390]\n",
            " [ 1344   327]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='era_')\n",
        "x_test2 = x_test2.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 era only random split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9458940702768001\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.553399578196922 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5896793874976332 0.5217325321913383 0.8002536461636017 0.5580165981481211 0.13867621082462123 0.24117295032914424\n",
            "[[10955  1567]\n",
            " [ 1268   403]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='nlp|era_')\n",
        "x_test2 = x_test2.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 weather + nlp random split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6842873782519863\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5672255778630952 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5711314692962648 0.5672255778630952 0.8505601352779539 0.5582635602632007 0.1468530964170436 0.17594254937163376\n",
            "[[11778   744]\n",
            " [ 1377   294]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='stat|nlp')\n",
        "x_test2 = x_test2.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 stat + nlp random split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8127488503690614\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5592267085548714 0.6\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5963065746357028 0.5559794276712192 0.8062425139153103 0.562966378455785 0.14168898448558792 0.24476361460203472\n",
            "[[11034  1488]\n",
            " [ 1262   409]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + WEATHER\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train2 = x_train2.filter(regex='stat|era_')\n",
        "x_test2 = x_test2.filter(regex='stat|era_')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 stat + weather random split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.114684\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.117734\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9455553518799874\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.597255060651974 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.7624891621028259 0.597255060651974 0.8218135700697526 0.6039430924732255 0.1685438687532862 0.3189706762417714\n",
            "[[11131  1391]\n",
            " [ 1138   533]]\n"
          ]
        }
      ],
      "source": [
        "### ALL VARS\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 all vars random split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n = 2, chrono split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 2\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "\n",
        "split_year = 2013\n",
        "years = x['year']\n",
        "\n",
        "train_mask = years <= split_year\n",
        "test_mask = years > split_year\n",
        "\n",
        "x_train2, x_test2 = x[train_mask], x[test_mask]\n",
        "y_train2, y_test2 = y[train_mask], y[test_mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7963883426026674\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.19859023806357232 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5559340680356192 0.19859023806357232 0.22248995983935743 0.5085560372377099 0.20318224720742303 0.9859719438877755\n",
            "[[  62 1929]\n",
            " [   7  492]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train2_stat = x_train2.filter(regex='stat|year')\n",
        "x_test2_stat = x_test2.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_stat, y_train2, x_test2_stat)\n",
        "results['n=2 stats only chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6608825348578294\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.4997429009139511 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.4915224723681416 0.4997429009139511 0.6044176706827309 0.5153607063448846 0.20567784701140499 0.3667334669338677\n",
            "[[1322  669]\n",
            " [ 316  183]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train2_nlp = x_train2.filter(regex='nlp')\n",
        "x_test2_nlp = x_test2.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_nlp, y_train2, x_test2_nlp)\n",
        "results['n=2 nlp only chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7779844335776264\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.27815555231285566 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5384792689346548 0.27815555231285566 0.2819277108433735 0.5074287198203539 0.2028148138542508 0.8837675350701403\n",
            "[[ 261 1730]\n",
            " [  58  441]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train2_era = x_train2.filter(regex='era_')\n",
        "x_test2_era = x_test2.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_era, y_train2, x_test2_era)\n",
        "results['n=2 era only chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8945316667886869\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.3412901487843572 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5241145273973362 0.3412901487843572 0.3417670682730924 0.5080542803336456 0.20302588327553922 0.7855711422845691\n",
            "[[ 459 1532]\n",
            " [ 107  392]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train2_wn = x_train2.filter(regex='nlp|era_')\n",
        "x_test2_wn = x_test2.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_wn, y_train2, x_test2_wn)\n",
        "results['n=2 weather + nlp chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6762226129676429\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5456656603523206 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5156948754364581 0.5456656603523206 0.7614457831325301 0.5437212949253605 0.2232841733119397 0.18036072144288579\n",
            "[[1806  185]\n",
            " [ 409   90]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train2_sn = x_train2.filter(regex='stat|nlp')\n",
        "x_test2_sn = x_test2.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_sn, y_train2, x_test2_sn)\n",
        "results['n=2 stat + nlp chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7936291197624896\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.48962573963773043 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.523044079117552 0.48962573963773043 0.5795180722891566 0.5118046238131713 0.20437275376040548 0.39879759519038077\n",
            "[[1244  747]\n",
            " [ 300  199]]\n"
          ]
        }
      ],
      "source": [
        "x_train2_se = x_train2.filter(regex='stat|era')\n",
        "x_test2_se = x_test2.filter(regex='stat|era')\n",
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2_se, y_train2, x_test2_se)\n",
        "results['n=2 stat + era chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_2    0.110888\n",
            "dtype: float64\n",
            "data imbalance test target_flood_2    0.200402\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8415880098659712\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.3923894372700348 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.523195059128805 0.3923894372700348 0.40602409638554215 0.4934212976430007 0.19832738883876738 0.6392785571142284\n",
            "[[ 692 1299]\n",
            " [ 180  319]]\n"
          ]
        }
      ],
      "source": [
        "print(\"data imbalance train\", y_train2.sum()/len(y_train2))\n",
        "print(\"data imbalance test\", y_test2.sum()/len(y_test2))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train2, y_train2, x_test2)\n",
        "results['n=2 all vars chrono split'] = utils.get_scores_clf(y_test2, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n = 5, random split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 5\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8563974997436457\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.6193620013919782 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.8162112347894114 0.6193620013919782 0.6326788636025584 0.73467759079584 0.3781665059592997 0.9304347826086956\n",
            "[[5511 4715]\n",
            " [ 224 2996]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='stat|year')\n",
        "x_test5 = x_test5.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 stats only random split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6613184142357336\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.4256515345607714 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.6045099387385461 0.4256515345607714 0.42867767365759335 0.5365151914557097 0.2538511203363307 0.7434782608695653\n",
            "[[3370 6856]\n",
            " [ 826 2394]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='nlp')\n",
        "x_test5 = x_test5.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 nlp only random split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9101930433320391\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5124947246751776 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.6287087596711828 0.5124947246751776 0.5314591700133868 0.588449519128564 0.2793895669234604 0.6978260869565217\n",
            "[[4899 5327]\n",
            " [ 973 2247]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='era_')\n",
        "x_test5 = x_test5.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 era only random split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9111032323030295\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.534974225447115 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.6414696644650768 0.534974225447115 0.5635133125092965 0.5963315710896473 0.2845920560684741 0.6593167701863354\n",
            "[[5454 4772]\n",
            " [1097 2123]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='nlp|era_')\n",
        "x_test5 = x_test5.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 weather + nlp random split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6754578785338157\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.38673124527018665 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.611692625544678 0.38673124527018665 0.38673211363974414 0.5317047156620622 0.251756169412337 0.8099378881987578\n",
            "[[2592 7634]\n",
            " [ 612 2608]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='stat|nlp')\n",
        "x_test5 = x_test5.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 stat + nlp random split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8971086285869415\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.5146023777716942 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.6341901747220883 0.5146023777716942 0.5358470920719917 0.5859087115658175 0.27818598275371614 0.6819875776397516\n",
            "[[5009 5217]\n",
            " [1024 2196]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + WEATHER\n",
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "x_train5 = x_train5.filter(regex='stat|era_')\n",
        "x_test5 = x_test5.filter(regex='stat|era_')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 stat + weather random split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.9382932996512136\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 10}\n",
            "maximum f1 score, thres 0.6923274473171055 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.8259883314119532 0.6923274473171055 0.7293618920124945 0.7530276314302964 0.41741077939616417 0.7984472049689441\n",
            "[[7236 2990]\n",
            " [ 649 2571]]\n"
          ]
        }
      ],
      "source": [
        "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.3, random_state=42) # random split first\n",
        "\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 all vars random split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### n=5, chrono split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pred = 5\n",
        "\n",
        "xy_df = attach_target(x_full, y_master, 'flood', n_pred)\n",
        "\n",
        "results={}\n",
        "\n",
        "x = xy_df.drop(xy_df.filter(regex='target|Unnamed').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "\n",
        "split_year = 2010\n",
        "years = x['year']\n",
        "\n",
        "train_mask = years <= split_year\n",
        "test_mask = years > split_year\n",
        "\n",
        "x_train5, x_test5 = x[train_mask], x[test_mask]\n",
        "y_train5, y_test5 = y[train_mask], y[test_mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8150050567559528\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.3028737604537326 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5338108746452408 0.3028737604537326 0.42730923694779116 0.5006856095857388 0.4264399996735692 0.9971724787935909\n",
            "[[   6 1423]\n",
            " [   3 1058]]\n"
          ]
        }
      ],
      "source": [
        "### STAT ONLY \n",
        "x_train5_stat = x_train5.filter(regex='stat|year')\n",
        "x_test5_stat = x_test5.filter(regex='stat|year')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_stat, y_train5, x_test5_stat)\n",
        "results['n=5 stats only chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.6552670827830225\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.29878907350042244 0.4\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.47055341455998645 0.29257448574854716 0.42610441767068274 0.5 0.42610441767068274 1.0\n",
            "[[   0 1429]\n",
            " [   0 1061]]\n"
          ]
        }
      ],
      "source": [
        "### NLP ONLY \n",
        "x_train5_nlp = x_train5.filter(regex='nlp')\n",
        "x_test5_nlp = x_test5.filter(regex='nlp')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_nlp, y_train5, x_test5_nlp)\n",
        "results['n=5 nlp only chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7423806283681639\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.34012273181260844 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.5068095970831747 0.34012273181260844 0.42329317269076305 0.48638575251175825 0.41955798564384095 0.9132893496701225\n",
            "[[  85 1344]\n",
            " [  92  969]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER ONLY\n",
        "x_train5_era = x_train5.filter(regex='era_')\n",
        "x_test5_era = x_test5.filter(regex='era_')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_era, y_train5, x_test5_era)\n",
        "results['n=5 era only chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7624065625407784\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.32759965682455966 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.47760704776314516 0.32759965682455966 0.41485943775100403 0.47855252283881283 0.41589133859837646 0.9095193213949104\n",
            "[[  68 1361]\n",
            " [  96  965]]\n"
          ]
        }
      ],
      "source": [
        "### WEATHER + NLP\n",
        "x_train5_wn = x_train5.filter(regex='nlp|era_')\n",
        "x_test5_wn = x_test5.filter(regex='nlp|era_')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_wn, y_train5, x_test5_wn)\n",
        "results['n=5 weather + nlp chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.671346399870154\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.3005391113753974 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.4741780104988296 0.3005391113753974 0.42610441767068274 0.49963592449126715 0.42592643061511937 0.9971724787935909\n",
            "[[   3 1426]\n",
            " [   3 1058]]\n"
          ]
        }
      ],
      "source": [
        "### STATS + NLP\n",
        "x_train5_sn = x_train5.filter(regex='stat|nlp')\n",
        "x_test5_sn = x_test5.filter(regex='stat|nlp')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_sn, y_train5, x_test5_sn)\n",
        "results['n=5 stat + nlp chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.7225396021774361\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.31509453884386557 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.511794859280199 0.31509453884386557 0.42248995983935744 0.4917538875943249 0.422110775249857 0.9604147031102733\n",
            "[[  33 1396]\n",
            " [  42 1019]]\n"
          ]
        }
      ],
      "source": [
        "x_train5_se = x_train5.filter(regex='stat|era')\n",
        "x_test5_se = x_test5.filter(regex='stat|era')\n",
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5_se, y_train5, x_test5_se)\n",
        "results['n=5 stat + era chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.232601\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.426104\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8558100654524949\n",
            "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'scale_pos_weight': 65}\n",
            "maximum f1 score, thres 0.3063862978984995 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.4823100195294852 0.3063862978984995 0.42530120481927713 0.49735847389044363 0.42481646777450255 0.9849198868991518\n",
            "[[  14 1415]\n",
            " [  16 1045]]\n"
          ]
        }
      ],
      "source": [
        "print(\"data imbalance train\", y_train5.sum()/len(y_train5))\n",
        "print(\"data imbalance test\", y_test5.sum()/len(y_test5))\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train5, y_train5, x_test5)\n",
        "results['n=5 all vars chrono split'] = utils.get_scores_clf(y_test5, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9X3yPye_oSZ",
        "outputId": "453781b8-324d-4225-dc0c-ae00793c5a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of xy_df 44820\n",
            "imbalance target_flood_5    0.243351\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Index(['grid_id', 'year', 'stat_flood_amt', 'stat_storm_amt',\n",
              "       'stat_earthquake_amt', 'stat_extreme temperature _amt',\n",
              "       'stat_landslide_amt', 'stat_volcanic activity_amt', 'stat_drought_amt',\n",
              "       'stat_mass movement (dry)_amt', 'stat_flood_ct', 'stat_storm_ct',\n",
              "       'stat_earthquake_ct', 'stat_extreme temperature _ct',\n",
              "       'stat_landslide_ct', 'stat_volcanic activity_ct', 'stat_drought_ct',\n",
              "       'stat_mass movement (dry)_ct', 'stat_flood_bin', 'stat_storm_bin',\n",
              "       'stat_earthquake_bin', 'stat_extreme temperature _bin',\n",
              "       'stat_landslide_bin', 'stat_volcanic activity_bin', 'stat_drought_bin',\n",
              "       'stat_mass movement (dry)_bin', 'stat_lat', 'stat_lon',\n",
              "       'target_flood_5'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#construct xy_df for appropriate prediction year, depending on the n_pred target period\n",
        "n_pred = 5\n",
        "\n",
        "#Riley: attach NLP and ERA features here\n",
        "\n",
        "# x_df = x_df.loc[x_df['year']>=1979] #crop to after 1979\n",
        "xy_df = attach_target(x_df, y_master, 'flood', n_pred)\n",
        "print('length of xy_df', len(xy_df))\n",
        "print('imbalance', xy_df.filter(regex='target').sum()/len(xy_df))\n",
        "\n",
        "xy_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxHMcTRB_yAg",
        "outputId": "fd692c14-59a2-4b98-9e17-e8ea621b9e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data imbalance train target_flood_5    0.245012\n",
            "dtype: float64\n",
            "data imbalance test target_flood_5    0.239476\n",
            "dtype: float64\n",
            "running xgb...\n",
            "[10, 20, 65]\n",
            "Train AUC:  0.8563974915055976\n",
            "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'scale_pos_weight': 20}\n",
            "maximum f1 score, thres 0.6193620013919782 0.7\n",
            "auc, f1, accu, accu_bl, precision, recall=  0.8162112347894114 0.6193620013919782 0.6326788636025584 0.73467759079584 0.3781665059592997 0.9304347826086956\n",
            "[[5511 4715]\n",
            " [ 224 2996]]\n"
          ]
        }
      ],
      "source": [
        "# Random splitting\n",
        "results={}\n",
        "# Separate features (X) and targets (y)\n",
        "x = xy_df.drop(xy_df.filter(regex='target').columns, axis=1)  # Drop target columns\n",
        "x = x.select_dtypes(['number'])  # Keep only numerical columns\n",
        "y = xy_df.filter(regex='target')  # Keep only target columns\n",
        "\n",
        "\n",
        "#train_test_split randomly\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "print(\"data imbalance train\", y_train.sum()/len(y_train))\n",
        "print(\"data imbalance test\", y_test.sum()/len(y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_pred_prob = utils.run_xgb(x_train, y_train, x_test)\n",
        "results['stats only random split'] = utils.get_scores_clf(y_test, y_pred_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzgNI-4t_plO"
      },
      "outputs": [],
      "source": [
        "#Riley:\n",
        "# - fix the linking problem of NLP features (pls use the old .pkl let's make sure we are using as raw as possible)\n",
        "# - attach era features\n",
        "# - compute results for random split, and non-random split: using the NEW attach_target function here\n",
        "# - for each, record the data imbalance issues.\n",
        "\n",
        "\n",
        "# - run results training using ALL data, and compute the locations with the highest risks of flooding with n_pred = 3, 5, that is 2021 and 2023 -> this is the closest to a \"live\" prediction we can do using our data.\n",
        "# - let's think about how to visualize the live.\n",
        "# - we should discuss the paper -> given current results, the paper is not very strong... option 1: we don't submit, keep working on it. option 2: we write something about the observations and try submit.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
